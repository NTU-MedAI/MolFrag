{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmilesPE Tokenizer\n",
    "\n",
    "> Tokenize SMILES (Simplified Molecular-Input Line-Entry System) into substructure units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import codecs\n",
    "import io\n",
    "import argparse\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "sys.path.append('..')\n",
    "from SmilesPE.pretokenizer import *\n",
    "from SmilesPE.learner import *\n",
    "\n",
    "\n",
    "class SPE_Tokenizer(object):\n",
    "    \"\"\"\n",
    "    Tokenize SMILES based on the learned SPE tokens.\n",
    "    \n",
    "    codes: output file of `learn_SPE()`\n",
    "    \n",
    "    merges: number of learned SPE tokens you want to use. `-1` means using all of them. `1000` means use the most frequent 1000.\n",
    "    \n",
    "    exclusive_tokens: argument that passes to  `atomwise_tokenizer()`\n",
    "    \n",
    "    glossaries: argument that passes to `isolate_glossary()`\n",
    "    \n",
    "    dropout: See [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267).\n",
    "    If `dropout` is set to 0, the segmentation is equivalent to the standard BPE; if `dropout` is set to 1, the segmentation splits words into distinct characters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, codes, merges=-1, glossaries=None, exclusive_tokens=None):\n",
    "\n",
    "        codes.seek(0)\n",
    "        offset=1\n",
    "\n",
    "        self.bpe_codes = [tuple(item.strip('\\r\\n ').split(' ')) for (n, item) in enumerate(codes) if (n < merges or merges == -1)]\n",
    "\n",
    "        for i, item in enumerate(self.bpe_codes):\n",
    "            if len(item) != 2:\n",
    "                sys.stderr.write('Error: invalid line {0} in BPE codes file: {1}\\n'.format(i+offset, ' '.join(item)))\n",
    "                sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\\n')\n",
    "                sys.exit(1)\n",
    "\n",
    "        # some hacking to deal with duplicates (only consider first instance)\n",
    "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
    "        \n",
    "        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n",
    "        \n",
    "        self.glossaries = glossaries if glossaries else []\n",
    "\n",
    "        self.glossaries_regex = re.compile('^({})$'.format('|'.join(glossaries))) if glossaries else None\n",
    "        \n",
    "        self.exclusive_tokens = exclusive_tokens\n",
    "        self.cache = {}\n",
    "    \n",
    "    def tokenize(self, smi, dropout=0):\n",
    "        segments = [out for segment in self._isolate_glossaries(smi)\n",
    "                    for out in encode(segment,\n",
    "                                      self.bpe_codes,\n",
    "                                      self.bpe_codes_reverse,\n",
    "                                      self.cache,\n",
    "                                      self.exclusive_tokens,\n",
    "                                      self.glossaries_regex,\n",
    "                                      dropout)]\n",
    "        return ' '.join(segments)\n",
    "        \n",
    "\n",
    "    def _isolate_glossaries(self, word):\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                                 for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments\n",
    "\n",
    "\n",
    "def encode(orig, bpe_codes, bpe_codes_reverse, cache, \n",
    "           exclusive_tokens=None, glossaries_regex=None, dropout=0):\n",
    "    \"\"\"Encode word based on list of SPE merge operations, which are applied consecutively.\n",
    "    \"\"\"\n",
    "\n",
    "    if not dropout and orig in cache:\n",
    "        return cache[orig]\n",
    "\n",
    "    if glossaries_regex and glossaries_regex.match(orig):\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    if len(orig) == 1:\n",
    "        return orig\n",
    "    \n",
    "    word = atomwise_tokenizer(orig, exclusive_tokens=exclusive_tokens)\n",
    "\n",
    "    while len(word) > 1:\n",
    "\n",
    "        # get list of symbol pairs; optionally apply dropout\n",
    "        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        #get first merge operation in list of BPE codes\n",
    "        bigram = min(pairs)[2]\n",
    "\n",
    "        # find start position of all pairs that we want to merge\n",
    "        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n",
    "\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        bigram = ''.join(bigram)\n",
    "        for j in positions:\n",
    "            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n",
    "            if j < i:\n",
    "                continue\n",
    "            new_word.extend(word[i:j]) # all symbols before merged pair\n",
    "            new_word.append(bigram) # merged pair\n",
    "            i = j+2 # continue after merged pair\n",
    "        new_word.extend(word[i:]) # add all symbols until end of word\n",
    "        word = new_word\n",
    "\n",
    "    word = tuple(word)\n",
    "\n",
    "    cache[orig] = word\n",
    "    return word\n",
    "\n",
    "def isolate_glossary(word, glossary):\n",
    "    \"\"\"\n",
    "    Isolate a glossary present inside a word.\n",
    "    \n",
    "    Returns a list of subwords. In which all 'glossary' glossaries are isolated.\n",
    "    \n",
    "    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n",
    "        ['1934', 'USA', 'B', 'USA']\n",
    "    \"\"\"\n",
    "    # regex equivalent of (if word == glossary or glossary not in word)\n",
    "    if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n",
    "        return [word]\n",
    "    else:\n",
    "        segments = re.split(r'({})'.format(glossary), word)\n",
    "        segments, ending = segments[:-1], segments[-1]\n",
    "        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n",
    "        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
