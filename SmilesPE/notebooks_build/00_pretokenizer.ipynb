{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Tokenizer\n",
    "\n",
    "> Tokenize SMILES (Simplified Molecular-Input Line-Entry System) into units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def atomwise_tokenizer(smi, exclusive_tokens = None):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule at atom-level:\n",
    "        (1) 'Br' and 'Cl' are two-character tokens\n",
    "        (2) Symbols with bracket are considered as tokens\n",
    "    \n",
    "    exclusive_tokens: A list of specifical symbols with bracket you want to keep. e.g., ['[C@@H]', '[nH]'].\n",
    "    Other symbols with bracket will be replaced by '[UNK]'. default is `None`.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    \n",
    "    if exclusive_tokens:\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok.startswith('['):\n",
    "                if tok not in exclusive_tokens:\n",
    "                    tokens[i] = '[UNK]'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def atomwise_tokenizer(smiles, exclusive_tokens = None):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule at atom-level:\n",
    "        (1) 'Br' and 'Cl' are two-character tokens\n",
    "        (2) Symbols with bracket are considered as tokens\n",
    "        (3) All other symbols are tokenized on character level.\n",
    "    \n",
    "    exclusive_tokens: A list of specifical symbols with bracket you want to keep. e.g., ['[C@@H]', '[nH]'].\n",
    "    Other symbols with bracket will be replaced by '[UNK]'. default is `None`.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from functools import reduce\n",
    "    regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "    char_list = re.split(regex, smiles)\n",
    "    tokens = []\n",
    "        \n",
    "    if exclusive_tokens:\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                if char in exclusive_tokens:\n",
    "                    tokens.append(str(char))\n",
    "                else:\n",
    "                    tokens.append('[UNK]')\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokens.append(i) for i in chars]                    \n",
    "        \n",
    "    if not exclusive_tokens:\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                tokens.append(str(char))\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokens.append(i) for i in chars]\n",
    "                \n",
    "    #fix the 'Br' be splited into 'B' and 'r'\n",
    "    if 'r' in tokens:\n",
    "        for index, tok in enumerate(tokens):\n",
    "            if tok == 'r':\n",
    "                if tokens[index-1] == 'B':\n",
    "                        tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
    "        \n",
    "    #fix the 'Cl' be splited into 'C' and 'l'\n",
    "    if 'l' in tokens:\n",
    "        for index, tok in enumerate(tokens):\n",
    "            if tok == 'l':\n",
    "                if tokens[index-1] == 'C':\n",
    "                        tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a SMILES string on atom-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', '[N+]', '(', 'C', ')', '(', 'C', ')', 'C', 'c', '1', 'c', 'c', 'c', 'c', 'c', '1', 'Br']\n"
     ]
    }
   ],
   "source": [
    "smi = 'CC[N+](C)(C)Cc1ccccc1Br'\n",
    "toks = atomwise_tokenizer(smi)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Br', 'B', 'C', '>', '>', '[dum]', 'c', 'o', 'b', 'c', 'Cl', '[Br]', '%11']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "atomwise_tokenizer('ABrBCD>>[dum]dumcobrclCl[Br] %11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a SMILES string on atom-level. Only include specifcal symbols in the `exclusive_tokens` list. The symbols with bracket which are not in `exclusive_tokens` will be replaced with `[UNK]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', '(', 'C', ')', 'C', '[C@@H]', '1', 'N', '2', 'C', '(', '=', 'O', ')', '[UNK]', '(', 'N', 'C', '(', '=', 'O', ')', '[UNK]', '3', 'C', 'N', '(', 'C', ')', '[C@@H]', '4', 'C', 'c', '5', 'c', '(', 'Br', ')', '[UNK]', 'c', '6', 'c', 'c', 'c', 'c', '(', 'C', '4', '=', 'C', '3', ')', 'c', '5', '6', ')', '(', 'O', '[C@@]', '2', '(', 'O', ')', '[C@@H]', '7', 'C', 'C', 'C', 'N', '7', 'C', '1', '=', 'O', ')', 'C', '(', 'C', ')', 'C']\n"
     ]
    }
   ],
   "source": [
    "sep_tokens = ['[C@@H]', '[C@@]']\n",
    "smi = 'CC(C)C[C@@H]1N2C(=O)[C@](NC(=O)[C@H]3CN(C)[C@@H]4Cc5c(Br)[nH]c6cccc(C4=C3)c56)(O[C@@]2(O)[C@@H]7CCCN7C1=O)C(C)C'\n",
    "toks = atomwise_tokenizer(smi, exclusive_tokens=sep_tokens)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'C', 'S']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "seq = 'ABCDTTDSE'\n",
    "toks = atomwise_tokenizer(seq)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def kmer_tokenizer(smiles, ngram=4, stride=1, remove_last = False, exclusive_tokens = None):\n",
    "    units = atomwise_tokenizer(smiles, exclusive_tokens = exclusive_tokens) #collect all the atom-wise tokens from the SMILES\n",
    "    if ngram == 1:\n",
    "        tokens = units\n",
    "    else: \n",
    "        tokens = [tokens_to_mer(units[i:i+ngram]) for i in range(0, len(units), stride) if len(units[i:i+ngram]) == ngram]\n",
    "    \n",
    "    if remove_last:\n",
    "        if len(tokens[-1]) < ngram: #truncate last whole k-mer if the length of the last k-mers is less than ngram.\n",
    "            tokens = tokens[:-1]\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_mer(toks):\n",
    "    return ''.join(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a SMILES string into 4-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC[N+](', 'C[N+](C', '[N+](C)', '(C)(', 'C)(C', ')(C)', '(C)C', 'C)Cc', ')Cc1', 'Cc1c', 'c1cc', '1ccc', 'cccc', 'cccc', 'ccc1', 'cc1Br']\n"
     ]
    }
   ],
   "source": [
    "smi = 'CC[N+](C)(C)Cc1ccccc1Br'\n",
    "toks = kmer_tokenizer(smi, ngram=4)\n",
    "print(toks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
