{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmilesPE Learner\n",
    "\n",
    "> Train a SmilesPE learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import copy\n",
    "import io\n",
    "import warnings\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from SmilesPE.pretokenizer import *\n",
    "\n",
    "def randomize_smiles(smiles):\n",
    "    \"\"\"\n",
    "    Require `RDKit` library. \n",
    "    \n",
    "    Generate a new SMILES string for the same molecule.\n",
    "    \n",
    "    Perform a randomization of a SMILES string must be RDKit sanitizable.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    from rdkit import Chem\n",
    "    \n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    ans = list(range(m.GetNumAtoms()))\n",
    "    np.random.shuffle(ans)\n",
    "    nm = Chem.RenumberAtoms(m,ans)\n",
    "    return Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False)\n",
    "\n",
    "def corpus_augment(infile, outdir, cycles):\n",
    "    '''\n",
    "    infile: line separated SMILES file\n",
    "    outdir: directory to save the  augmented SMILE file. \n",
    "        Each round of augmentation will save as a separated file, named as `infile_Ri`. \n",
    "    cycles: number of rounds for SMILES augmentation\n",
    "    '''\n",
    "    if cycles <= 0:\n",
    "        raise ValueError(\"Invalid option,  cycle should be larger than 0\")\n",
    "    \n",
    "    with open(infile, \"r\") as ins:\n",
    "        can_smiles = []\n",
    "        for line in ins:\n",
    "            can_smiles.append(line.split('\\n')[0])\n",
    "    \n",
    "    fname = os.path.basename(infile).split('.')[0]\n",
    "    ftype = os.path.basename(infile).split('.')[1]\n",
    "    \n",
    "    mb = master_bar(range(cycles))\n",
    "    for i in mb:\n",
    "        with open(f'{outdir}/{fname}_R{i}.{ftype}', 'a') as outfile:\n",
    "            for smi in progress_bar(can_smiles, parent=mb):\n",
    "                randomized_smi = randomize_smiles(smi)\n",
    "                outfile.write(randomized_smi + '\\n')\n",
    "\n",
    "def get_vocabulary(smiles, augmentation=0, exclusive_tokens = False):\n",
    "    \"\"\"Read text and return dictionary that encodes vocabulary\n",
    "    \"\"\"\n",
    "    print('Counting SMILES...')\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for i, smi in enumerate(smiles):\n",
    "        vocab[smi] += 1\n",
    "    \n",
    "    print(f'{len(vocab)} unique Canonical SMILES')\n",
    "    \n",
    "    if augmentation>0:\n",
    "        print(f'Augmenting SMILES...({augmentation} times)')\n",
    "        mb = master_bar(range(augmentation))\n",
    "        for i in mb:        \n",
    "            for smi in progress_bar(smiles, parent=mb):\n",
    "                randomized_smi = randomize_smiles(smi)\n",
    "                vocab[randomized_smi] += 1\n",
    "    \n",
    "        print(f'{len(vocab)} unique SMILES (Canonical + Augmented)')\n",
    "    return dict([(tuple(atomwise_tokenizer(x)) ,y) for (x,y) in vocab.items()])\n",
    "\n",
    "def update_pair_statistics(pair, changed, stats, indices):\n",
    "    \"\"\"Minimally update the indices and frequency of symbol pairs\n",
    "    if we merge a pair of symbols, only pairs that overlap with occurrences\n",
    "    of this pair are affected, and need to be updated.\n",
    "    \"\"\"\n",
    "    stats[pair] = 0\n",
    "    indices[pair] = defaultdict(int)\n",
    "    first, second = pair\n",
    "    new_pair = first+second\n",
    "    for j, word, old_word, freq in changed:\n",
    "\n",
    "        # find all instances of pair, and update frequency/indices around it\n",
    "        i = 0\n",
    "        while True:\n",
    "            # find first symbol\n",
    "            try:\n",
    "                i = old_word.index(first, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])\n",
    "            if i < len(old_word)-1 and old_word[i+1] == second:\n",
    "                # assuming a symbol sequence \"A B C\", if \"B C\" is merged, reduce the frequency of \"A B\"\n",
    "                if i:\n",
    "                    prev = old_word[i-1:i+1]\n",
    "                    stats[prev] -= freq\n",
    "                    indices[prev][j] -= 1\n",
    "                if i < len(old_word)-2:\n",
    "                    # assuming a symbol sequence \"A B C B\", if \"B C\" is merged, reduce the frequency of \"C B\".\n",
    "                    # however, skip this if the sequence is A B C B C, because the frequency of \"C B\" will be reduced by the previous code block\n",
    "                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n",
    "                        nex = old_word[i+1:i+3]\n",
    "                        stats[nex] -= freq\n",
    "                        indices[nex][j] -= 1\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                # find new pair\n",
    "                i = word.index(new_pair, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # assuming a symbol sequence \"A BC D\", if \"B C\" is merged, increase the frequency of \"A BC\"\n",
    "            if i:\n",
    "                prev = word[i-1:i+1]\n",
    "                stats[prev] += freq\n",
    "                indices[prev][j] += 1\n",
    "            # assuming a symbol sequence \"A BC B\", if \"B C\" is merged, increase the frequency of \"BC B\"\n",
    "            # however, if the sequence is A BC BC, skip this step because the count of \"BC BC\" will be incremented by the previous code block\n",
    "            if i < len(word)-1 and word[i+1] != new_pair:\n",
    "                nex = word[i:i+2]\n",
    "                stats[nex] += freq\n",
    "                indices[nex][j] += 1\n",
    "            i += 1\n",
    "            \n",
    "def get_pair_statistics(vocab):\n",
    "    \"\"\"Count frequency of all symbol pairs, and create index\"\"\"\n",
    "\n",
    "    # data structure of pair frequencies\n",
    "    stats = defaultdict(int)\n",
    "\n",
    "    #index from pairs to words\n",
    "    indices = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i, (word, freq) in enumerate(progress_bar(vocab)):\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            stats[prev_char, char] += freq\n",
    "            indices[prev_char, char][i] += 1\n",
    "            prev_char = char\n",
    "\n",
    "    return stats, indices\n",
    "\n",
    "def replace_pair(pair, vocab, indices):\n",
    "    \"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"\n",
    "    first, second = pair\n",
    "    pair_str = ''.join(pair)\n",
    "    pair_str = pair_str.replace('\\\\','\\\\\\\\')\n",
    "    changes = []\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
    "    if sys.version_info < (3, 0):\n",
    "        iterator = indices[pair].iteritems()\n",
    "    else:\n",
    "        iterator = indices[pair].items()\n",
    "    for j, freq in iterator:\n",
    "        if freq < 1:\n",
    "            continue\n",
    "        word, freq = vocab[j]\n",
    "        new_word = ' '.join(word)\n",
    "        new_word = pattern.sub(pair_str, new_word)\n",
    "        new_word = tuple(new_word.split(' '))\n",
    "\n",
    "        vocab[j] = (new_word, freq)\n",
    "        changes.append((j, new_word, word, freq))\n",
    "\n",
    "    return changes\n",
    "\n",
    "def prune_stats(stats, big_stats, threshold):\n",
    "    \"\"\"Prune statistics dict for efficiency of max()\n",
    "    The frequency of a symbol pair never increases, so pruning is generally safe\n",
    "    (until we the most frequent pair is less frequent than a pair we previously pruned)\n",
    "    big_stats keeps full statistics for when we need to access pruned items\n",
    "    \"\"\"\n",
    "    for item,freq in list(stats.items()):\n",
    "        if freq < threshold:\n",
    "            del stats[item]\n",
    "            if freq < 0:\n",
    "                big_stats[item] += freq\n",
    "            else:\n",
    "                big_stats[item] = freq\n",
    "\n",
    "def learn_SPE(infile, outfile, num_symbols, min_frequency=2, augmentation=0, verbose=False, total_symbols=False):\n",
    "    \"\"\"\n",
    "    Learn num_symbols SPE operations from infile and write to outfile.\n",
    "    \n",
    "    *infile*: a list of SMILES\n",
    "    \n",
    "    *num_symbols*: maximum total number of SPE symbols \n",
    "    \n",
    "    *min_frequency*: the minimum frequency of SPE symbols appears.\n",
    "    \n",
    "    *augmentation*: times of SMILES augmentation\n",
    "    \n",
    "    *verbose*: if True, print the merging process\n",
    "    \n",
    "    *total_symbols*: if True; the maximum total of SPE symbols = num_symbols - number of atom-level tokens\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    vocab = get_vocabulary(infile, augmentation=augmentation)\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print('Gettting Pair Statistics')\n",
    "    stats, indices = get_pair_statistics(sorted_vocab)\n",
    "    big_stats = copy.deepcopy(stats)\n",
    "\n",
    "    if total_symbols:\n",
    "        uniq_char = set()\n",
    "        for word in vocab:\n",
    "            for char in word:\n",
    "                uniq_char.add(char)\n",
    "        sys.stderr.write(f'Number of unique characters & Reducing number of merge operations by: {len(uniq_char)}\\n')\n",
    "        sys.stderr.write(f'Unique characters: {(uniq_char)}\\n')\n",
    "        num_symbols -= len(uniq_char)\n",
    "                    \n",
    "    # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "    threshold = max(stats.values()) / 10\n",
    "    for i in range(num_symbols):\n",
    "        if stats:\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "\n",
    "        # we probably missed the best pair because of pruning; go back to full statistics\n",
    "        if not stats or (i and stats[most_frequent] < threshold):\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "            stats = copy.deepcopy(big_stats)\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "            # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "            threshold = stats[most_frequent] * i/(i+10000.0)\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "        if stats[most_frequent] < min_frequency:\n",
    "            sys.stderr.write('no pair has frequency >= {0}. Stopping\\n'.format(min_frequency))\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n",
    "        outfile.write('{0} {1}\\n'.format(*most_frequent))\n",
    "        changes = replace_pair(most_frequent, sorted_vocab, indices)\n",
    "        update_pair_statistics(most_frequent, changes, stats, indices)\n",
    "        stats[most_frequent] = 0\n",
    "        if not i % 100:\n",
    "            prune_stats(stats, big_stats, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "infile = '../experiments/data/smiles_toy.smi'\n",
    "outdir = '../experiments/data/aug_chembl/'\n",
    "\n",
    "corpus_augment(infile, outdir, cycles=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
